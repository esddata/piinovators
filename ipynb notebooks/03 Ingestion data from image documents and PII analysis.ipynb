{"cells":[{"cell_type":"markdown","source":["# Ingestion data from image documents and PII analysis using OpenAI service\n","##### using GPT4 version 1106-Preview to find PII data, classify if that image is a complaint or not, and do the categorization of the image"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"84fbc10f-53c8-4ab6-b284-63d893a66e50"},{"cell_type":"markdown","source":["**_check OpenAI version_**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"600b84f0-5f60-435e-ac6d-2c47ec7ff7cf"},{"cell_type":"code","source":["#%pip install openai --upgrade\n","\n","def check_openai_version():\n","    \"\"\"\n","    Check Azure Open AI version\n","    \"\"\"\n","    import openai\n","\n","    installed_version = openai.__version__\n","\n","    try:\n","        version_number = float(installed_version[:3])\n","    except ValueError:\n","        print(\"Invalid OpenAI version format\")\n","        return\n","\n","    print(f\"Installed OpenAI version: {installed_version}\")\n","\n","    if version_number < 1.0:\n","        print(\"[Warning] You should upgrade OpenAI to have version >= 1.0.0\")\n","        print(\"To upgrade, run: %pip install openai --upgrade\")\n","    else:\n","        print(f\"[OK] OpenAI version {installed_version} is >= 1.0.0\")\n","\n","\n","check_openai_version()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"session_starting","livy_statement_state":null,"queued_time":"2024-03-02T22:32:05.2767692Z","session_start_time":"2024-03-02T22:32:06.2544824Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"329ef813-9333-4312-895a-bb1215466a39"},"text/plain":"StatementMeta(, , , SessionStarting, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7a200859-6c8a-4614-8d8e-5fc3671d9e4f"},{"cell_type":"markdown","source":["_**install needed libraries**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f65498e1-1728-4586-a375-e85d96b8722b"},{"cell_type":"code","source":["import datetime\n","import openai\n","import os\n","import base64\n","import requests\n","import json\n","import sys\n","\n","#from dotenv import load_dotenv\n","import openai\n","from IPython.display import Image\n","import time\n","\n","from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n","from msrest.authentication import CognitiveServicesCredentials\n","from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n","#from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e9be5ed6-cfa8-4977-bfe0-b7dbc3c32126"},{"cell_type":"markdown","source":["_**Connecting OpenAI service using key vault secrets**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d739ac15-7b5c-430e-91b5-60888230eef0"},{"cell_type":"code","source":["from notebookutils.mssparkutils.credentials import getSecret\n","\n","KEYVAULT_ENDPOINT = \"https://mfaiFabricKeyVault.vault.azure.net/\"\n","\n","AZURE_OPENAI_KEY=getSecret(KEYVAULT_ENDPOINT, \"openaiKeyGPT4Vision\")\n","AZURE_OPENAI_ENDPOINT=getSecret(KEYVAULT_ENDPOINT, \"openaiEndpointGPT4Vision\")\n","\n","openai.api_type = 'azure'\n","openai.api_key = AZURE_OPENAI_KEY\n","openai.api_base = AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n","openai.api_version = '2023-05-15' # this might change in the future\n","\n","model = \"gpt-4-vision\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"48d350d6-eff7-4051-9b48-0a7d0bdfc7be"},{"cell_type":"markdown","source":["_**Connecting computer-vision service using key vault secrets**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9100b226-5926-4c94-9add-d6432cedaa3f"},{"cell_type":"code","source":["VISION_KEYVAULT_ENDPOINT = \"https://computer-vision-service.vault.azure.net/\"\n","\n","VISION_OPENAI_KEY=getSecret(VISION_KEYVAULT_ENDPOINT, \"computer-vision-key\")\n","VISION_OPENAI_ENDPOINT=getSecret(VISION_KEYVAULT_ENDPOINT, \"computer-vision-endpoint\")\n","\n","key=VISION_OPENAI_KEY\n","endpoint = VISION_OPENAI_ENDPOINT\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"72c98507-8eba-4b19-b53c-0af059184961"},{"cell_type":"markdown","source":["_**Creates a client using the computer vision service given an endpoint**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"05573dc3-fd9b-4dd2-b280-ab733668209d"},{"cell_type":"code","source":["computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(key))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"02ba36b8-680a-445e-938e-f939fabfa1fe"},{"cell_type":"markdown","source":["_**definition gpt4V function which returns responses in JSON format**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1f5927ee-2485-4cdc-ab43-66ad198b0efd"},{"cell_type":"code","source":["def gpt4V(image_file, query):\n","    \"\"\"\n","    GPT4-Vision\n","    \"\"\"\n","    # Endpoint\n","    base_url = f\"{openai.api_base}/openai/deployments/{model}\"\n","    endpoint = f\"{base_url}/chat/completions?api-version=2023-12-01-preview\"\n","\n","    # Header\n","    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai.api_key}\n","\n","    # Encoded image\n","    base_64_encoded_image = base64.b64encode(open(image_file, \"rb\").read()).decode(\n","        \"ascii\"\n","    )\n","\n","    # Prompt\n","    data = {\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant, and you only replay with JSON.\"},\n","            {\"role\": \"user\", \"content\": [query, {\"image\": base_64_encoded_image}]},\n","        ],\n","        \"max_tokens\": 4000,\n","    }\n","\n","    # Results\n","    response = requests.post(endpoint, headers=headers, data=json.dumps(data))\n","\n","    if response.status_code == 200:\n","        result = json.loads(response.text)[\"choices\"][0][\"message\"][\"content\"]\n","        return result\n","    \n","    if response.status_code == 429:\n","        print(\"[ERROR] Too many requests. Please wait a couple of seconds and try again.\")\n","    \n","    else:\n","        print(\"[ERROR] Error code:\", response.status_code)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"57ece862-8cea-4e2a-a2a6-3228139c5e77"},{"cell_type":"markdown","source":["_**definition gpt4V_array function which returns responses in array**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fc5d97cd-68a3-44cd-9785-ec66e7c33645"},{"cell_type":"code","source":["def gpt4V_array(image_file, query):\n","    \"\"\"\n","    GPT4-Vision\n","    \"\"\"\n","    # Endpoint\n","    base_url = f\"{openai.api_base}/openai/deployments/{model}\"\n","    endpoint = f\"{base_url}/chat/completions?api-version=2023-12-01-preview\"\n","\n","    # Header\n","    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai.api_key}\n","\n","    # Encoded image\n","    base_64_encoded_image = base64.b64encode(open(image_file, \"rb\").read()).decode(\n","        \"ascii\"\n","    )\n","\n","    # Prompt\n","    data = {\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant, and you only replay with array.\"},\n","            {\"role\": \"user\", \"content\": [query, {\"image\": base_64_encoded_image}]},\n","        ],\n","        \"max_tokens\": 4000,\n","    }\n","\n","    # Results\n","    response = requests.post(endpoint, headers=headers, data=json.dumps(data))\n","\n","    if response.status_code == 200:\n","        result = json.loads(response.text)[\"choices\"][0][\"message\"][\"content\"]\n","        return result\n","    \n","    if response.status_code == 429:\n","        print(\"[ERROR] Too many requests. Please wait a couple of seconds and try again.\")\n","    \n","    else:\n","        print(\"[ERROR] Error code:\", response.status_code)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8115a52c-c9ff-46f8-b8f7-2eb88f99705a"},{"cell_type":"code","source":["from IPython.display import Image\n","\n","imagefile = f\"/lakehouse/default/Files/bronze/raw/unprocessed/images/{DocumentNameFinal}\"\n","\n","PIIdataarray=[]\n","\n","Image(filename=imagefile)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"a3f1e7aa-5480-4029-9047-22e80b08924f"},{"cell_type":"code","source":["#prompt (needed for document clasiffication is complaint or not)\n","time.sleep(60)\n","result = gpt4V(imagefile, \"Is this document a complaint or not? Answer only with yes or no.\")\n","print(result)\n","\n","time.sleep(40)\n","columns=[\"DocumentID\",\"response\",\"UpdatedAt\"]\n","spark.createDataFrame([(DocumentID, result, UpdatedAt)],columns).write.format(\"delta\").mode(\"append\").saveAsTable(\"clasiffication\")\n","\n","spark.sql(\"UPDATE PIInovatorsLH.clasiffication  set response=REPLACE(REPLACE(response,'```json',''),'```','') WHERE DocumentID={parDocumentID}\",parDocumentID = DocumentID)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"bb08c8c9-bf02-41da-9532-2fe4b5f0182c"},{"cell_type":"code","source":["#prompt (needed to return PII data in image to be blured)\n","time.sleep(60)\n","result = gpt4V(imagefile, \"What are the Personally identifiable information in this image? Return only adresses, emails, full names, accounts, tepephones.\")\n","text_from_image= result\n","print(result)\n","\n","time.sleep(40)\n","columns=[\"DocumentID\",\"response\",\"UpdatedAt\"]\n","spark.createDataFrame([(DocumentID, result, UpdatedAt)],columns).write.format(\"delta\").mode(\"append\").saveAsTable(\"pii_data\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d1aa6cda-e651-4158-93c0-c43922d0a3ae"},{"cell_type":"code","source":["#prompt (needed to return senders PII data)\n","time.sleep(60)\n","result = gpt4V(imagefile, \"Who is the sender and from which town and country and which is sender's mail and phone?\")\n","\n","time.sleep(40)\n","columns=[\"DocumentID\",\"response\",\"UpdatedAt\"]\n","spark.createDataFrame([(DocumentID, result, UpdatedAt)],columns).write.format(\"delta\").mode(\"append\").saveAsTable(\"sender_data\")\n","\n","spark.sql(\"UPDATE PIInovatorsLH.sender_data  set response=REPLACE(REPLACE(response,'```json',''),'```','') WHERE DocumentID={parDocumentID}\",parDocumentID = DocumentID)\n","\n","print(result)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d0ac8b44-149c-40c7-9b38-370cb81aa496"},{"cell_type":"code","source":["#prompt (needed to categorized the complaint)\"\n","time.sleep(60)\n","result = gpt4V(imagefile, \"In which complaint category is the text in this image? Posible categories are:Product or service,Wait time\"\n","\"Delivery,Personnel,Online,Continual,Communication. Return only one category.\")\n","\n","time.sleep(40)\n","columns=[\"DocumentID\",\"response\",\"UpdatedAt\"]\n","spark.createDataFrame([(DocumentID, result, UpdatedAt)],columns).write.format(\"delta\").mode(\"append\").saveAsTable(\"category\")\n","\n","spark.sql(\"UPDATE PIInovatorsLH.category  set response=REPLACE(REPLACE(response,'```json',''),'```','') WHERE DocumentID={parDocumentID}\",parDocumentID = DocumentID)\n","\n","print(result)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"92074f4f-a8d9-4260-8bfd-c45b99c39746"},{"cell_type":"code","source":["#prompt (needed to extract the subject of complaint)\n","time.sleep(60)\n","result = gpt4V(imagefile, \"What is the subject of the text from the image? Return only subject.\")\n","time.sleep(40)\n","columns=[\"DocumentID\",\"response\",\"UpdatedAt\"]\n","spark.createDataFrame([(DocumentID, result, UpdatedAt)],columns).write.format(\"delta\").mode(\"append\").saveAsTable(\"subject\")\n","\n","spark.sql(\"UPDATE PIInovatorsLH.subject  set response=REPLACE(REPLACE(response,'```json',''),'```','') WHERE DocumentID={parDocumentID}\",parDocumentID = DocumentID)\n","\n","print(result)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"778de7b4-fd4a-4701-85b7-24214d4915ed"},{"cell_type":"markdown","source":["_**start the process for blurring the PII data in the image **_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cd24b652-8c37-42c3-8103-391b32909eba"},{"cell_type":"code","source":["#prompt (mask the extracted PII data)\n","time.sleep(100)\n","result = gpt4V(imagefile, \" What are the Personally identifiable information of the sender only in this text ?\\\n","Return the same text with masked PII data (Masked PII data means display only the first char of all of the Personally \\\n","identifiable information and mask the rest of the chars with * for adresses, emails, full name, date of birth, telephone \\\n","numbers, driver's license number, credit or debit card number or Social Security number of the sender)\")\n","\n","columns=[\"DocumentID\",\"response\",\"UpdatedAt\"]\n","spark.createDataFrame([(DocumentID, result, UpdatedAt)],columns).write.format(\"delta\").mode(\"append\").saveAsTable(\"mask_pii_data\")\n","\n","print(result)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c90614b1-d6b2-4a65-ae3a-cce65036f7d8"},{"cell_type":"code","source":["#prompt (extract the PII data in an array)\n","time.sleep(90)\n","result = gpt4V_array(imagefile, \"What are the Personally identifiable information of the sender in this image?\\\n","Return them in an array including any punctuation marks which come after them in the text.\")\n","text_from_image= result\n","\n","print (result)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"999ed9d6-4eab-44ce-bafa-ec1bd217f1df"},{"cell_type":"markdown","source":["**_**Using Azure computer vision service for OCR and OpenCv library for bluring the PII data in the image **_**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2161bbc1-e699-44c7-ba1e-e40da1f67278"},{"cell_type":"code","source":["from PIL import Image\n","import numpy as np\n","import time\n","import cv2\n","from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n","from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n","from msrest.authentication import CognitiveServicesCredentials\n","\n","# Replace with your own values\n","endpoint = endpoint\n","subscription_key = key\n","computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n","\n","# Open local image file\n","\n","imagefile = f\"/lakehouse/default/Files/bronze/raw/unprocessed/images/{DocumentNameFinal}\"\n","image_path = imagefile\n","image = open(image_path, \"rb\")\n","img = Image.open(image_path)\n","original_img = img.copy()\n","\n","# Define the words to blur (replace this with your array)\n","words_to_blur = text_from_image\n","\n","# Call the API\n","read_response = computervision_client.read_in_stream(image, raw=True)\n","\n","# Get the operation location (URL with an ID at the end)\n","read_operation_location = read_response.headers[\"Operation-Location\"]\n","\n","# Grab the ID from the URL\n","operation_id = read_operation_location.split(\"/\")[-1]\n","\n","# Retrieve the results \n","while True:\n","    read_result = computervision_client.get_read_result(operation_id)\n","    if read_result.status not in ['notStarted', 'running']:\n","        break\n","    time.sleep(1)\n","\n","# Check if 'read_result' is defined\n","if hasattr(read_result, 'analyze_result') and hasattr(read_result.analyze_result, 'read_results'):\n","    for text_result in read_result.analyze_result.read_results:\n","        for line in text_result.lines:\n","            for word in line.words:\n","                if word.text in words_to_blur:\n","                    # Get the bounding box of the word\n","                    xy1 = [int(coord) for coord in word.bounding_box[0:2]]\n","                    xy3 = [int(coord) for coord in word.bounding_box[4:6]]\n","\n","                    # Apply Gaussian blur to that region\n","                    word_region = np.array(img)[xy1[1]:xy3[1], xy1[0]:xy3[0]]\n","                    blurred_word_region = cv2.GaussianBlur(word_region, (25, 25), 7)\n","                    img.paste(Image.fromarray(blurred_word_region), (xy1[0], xy1[1]))\n","else:\n","    print(\"Error: No analyze result found in read_result.\")\n","\n","# Display the modified image\n","img.show()\n","#original_img.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"055e6585-898f-4d39-ba8a-e579a13bfb21"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"52776468-6f4f-40e7-8534-dc45c02193d6","default_lakehouse_name":"PIInovatorsLH","default_lakehouse_workspace_id":"8917cd64-1bc0-4858-a527-045ba726753a","known_lakehouses":[{"id":"52776468-6f4f-40e7-8534-dc45c02193d6"}]},"environment":{"environmentId":"53559c07-b5c1-4a02-b66e-aa0e7684ad14","workspaceId":"8917cd64-1bc0-4858-a527-045ba726753a"}}},"nbformat":4,"nbformat_minor":5}